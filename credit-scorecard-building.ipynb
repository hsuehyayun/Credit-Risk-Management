{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 370089,
     "sourceType": "datasetVersion",
     "datasetId": 902
    }
   ],
   "dockerImageVersionId": 31259,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Credit Risk Modeling with Lending Club Data\n**Course Assignment — 2,000-word analytical report**\n\nThis notebook builds a credit risk scorecard using Lending Club personal loan data (2007–2018). The analysis covers three graded areas:\n\n| Section | Topic | Weight |\n|---------|-------|--------|\n| 1 | Data Preparation | 30 % |\n| 2 | Model Development | 60 % |\n| 3 | Limitations | 10 % |\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 1. Data Preparation\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.1 Data Source & Sample Selection\n\nThe dataset is the **Lending Club loan book 2007–2018 Q4**, downloaded from Kaggle (`lending-club`). The raw accepted-loans file contains 678,151 observations across 151 columns. A **30 % stratified random sample** was drawn at read-time to stay within memory limits, yielding roughly 203,000 rows before status filtering.\n\nLoans whose outcome had not yet been determined at extraction time were excluded:\n\n- **Current** — still repaying, no outcome known  \n- **In Grace Period** — overdue < 15 days, outcome uncertain  \n\nAfter exclusion the **final modelling sample is 412,053 loans with a Bad Rate of 21.4 %**.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# check data\n",
    "for f in os.listdir('/kaggle/input/lending-club'):\n",
    "    size = os.path.getsize(f'/kaggle/input/lending-club/{f}')\n",
    "    print(f\"{f}: {size/1e9:.2f} GB\")\n",
    "\n",
    "import random\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the accepted sampling rate since sufficient RAM is available\n",
    "# Accepted loans are fewer in number, so a higher sample rate improves coverage\n",
    "\n",
    "sample_rate_accepted = 0.3   # Sample 30% of accepted loans\n",
    "sample_rate_rejected = 0.05  # Keep rejected loans at 5% (for reference only)\n",
    "\n",
    "df_accepted = pd.read_csv(\n",
    "    '/kaggle/input/lending-club/accepted_2007_to_2018Q4.csv.gz',\n",
    "    skiprows=lambda x: x > 0 and random.random() > sample_rate_accepted,\n",
    "    compression='gzip',\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "df_rejected = pd.read_csv(\n",
    "    '/kaggle/input/lending-club/rejected_2007_to_2018Q4.csv.gz',\n",
    "    skiprows=lambda x: x > 0 and random.random() > sample_rate_rejected,\n",
    "    compression='gzip',\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "print(f\"Accepted: {len(df_accepted)} rows\")\n",
    "print(f\"Rejected: {len(df_rejected)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Target Variable Definition\n\nA binary target is constructed from `loan_status`:\n\n| Label | Value | Loan Statuses |\n|-------|-------|---------------|\n| **Bad** | 1 | Charged Off · Default · Late (31-120 days) · Late (16-30 days) · Does not meet credit policy: Charged Off |\n| **Good** | 0 | Fully Paid · Does not meet credit policy: Fully Paid |\n\nThe \"Does not meet credit policy\" variants are mapped identically to their base status because the policy override is a platform flag unrelated to borrower performance.  \nRows with statuses other than the above are dropped (Current, In Grace Period) to ensure every observation has a definitive outcome.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Accepted Loans — Columns ===\")\n",
    "print(df_accepted.columns.tolist())\n",
    "print(f\"\\nTotal columns: {df_accepted.shape[1]}\")\n",
    "\n",
    "print(\"\\n=== Rejected Loans — Columns ===\")\n",
    "print(df_rejected.columns.tolist())\n",
    "print(f\"\\nTotal columns: {df_rejected.shape[1]}\")\n",
    "\n",
    "# Find columns common to both datasets\n",
    "common_cols = set(df_accepted.columns) & set(df_rejected.columns)\n",
    "print(f\"\\nShared columns: {common_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case-insensitive column comparison\n",
    "accepted_cols = set(c.lower().replace(' ', '_') for c in df_accepted.columns)\n",
    "rejected_cols = set(c.lower().replace(' ', '_') for c in df_rejected.columns)\n",
    "print(accepted_cols & rejected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect loan_status distribution\n",
    "print(df_accepted['loan_status'].value_counts())\n",
    "print()\n",
    "print(df_accepted['loan_status'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude 'Current' and 'In Grace Period' — outcomes not yet determined\n",
    "# Map remaining statuses to Good (0) and Bad (1)\n",
    "\n",
    "bad_statuses = [\n",
    "    'Charged Off',\n",
    "    'Default',\n",
    "    'Late (31-120 days)',\n",
    "    'Late (16-30 days)',\n",
    "    'Does not meet the credit policy. Status:Charged Off'\n",
    "]\n",
    "\n",
    "good_statuses = [\n",
    "    'Fully Paid',\n",
    "    'Does not meet the credit policy. Status:Fully Paid'\n",
    "]\n",
    "\n",
    "# Filter rows and define binary target variable\n",
    "df_model = df_accepted[df_accepted['loan_status'].isin(bad_statuses + good_statuses)].copy()\n",
    "df_model['target'] = df_model['loan_status'].isin(bad_statuses).astype(int)\n",
    "\n",
    "# Inspect results\n",
    "print(f\"Modelling sample size: {len(df_model)}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_model['target'].value_counts())\n",
    "print()\n",
    "print(df_model['target'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 Feature Engineering & Preprocessing\n\n### Data Leakage Removal\n\nThe accepted-loan file contains 151 columns, but **125+ of them are only populated after loan origination** (e.g., `total_pymnt`, `recoveries`, `last_pymnt_amnt`, `last_fico_range_*`). Including any of these would constitute data leakage — the model would be \"predicting\" outcomes using information that is generated by those very outcomes. All post-origination columns are excluded; only **application-time features** are retained.\n\n### Missing Value Handling — Three Strategies\n\n| Strategy | Variables | Rationale |\n|----------|-----------|-----------|\n| Fill with **0** | `mort_acc`, `pub_rec_bankruptcies` | Missing almost certainly means \"none on record\" — a zero is the meaningful value |\n| Create **\"Unknown\" category** | `emp_length` | Missingness may carry predictive signal (e.g., self-employed, unemployed); masking it with a median would destroy that signal |\n| Fill with **median** | `annual_inc`, `dti`, `delinq_2yrs`, `inq_last_6mths`, `open_acc`, `pub_rec`, `total_acc`, `revol_util` | Fewer than 120 rows affected; median is robust to outliers |\n\nOne row missing `zip_code` is dropped outright (negligible loss).\n\n### Categorical Variable Consolidation\n\n`zip_code` (840+ unique values) and `sub_grade` (35 values, highly collinear with `grade`) are dropped.  \n`home_ownership` categories **ANY** and **NONE** are merged into **OTHER** because each represents < 0.1 % of observations — too sparse for stable WoE estimation.\n\n### High-Correlation Variable Removal\n\nThree pairs with Pearson |r| > 0.80 were identified:\n\n| Pair | Correlation | Decision |\n|------|-------------|----------|\n| `fico_range_low` / `fico_range_high` | ≈ 1.00 | Drop `fico_range_high` — identical information |\n| `loan_amnt` / `installment` | > 0.95 | Drop `installment` — `loan_amnt` is the primary underwriting variable |\n| `grade_WoE` / `int_rate` | > 0.90 | Drop `int_rate` — `grade_WoE` is already linearity-transformed and more model-friendly |\n\n### WoE Encoding & IV Filtering\n\nWeight of Evidence (WoE) encoding converts categorical variables into continuous risk scores, making them directly usable in logistic regression without creating dummy-variable multicollinearity.\n\n$$WoE_i = \\ln\\left(\\frac{\\text{Good}_i / \\text{Total Good}}{\\text{Bad}_i / \\text{Total Bad}}\\right)$$\n\n$$IV = \\sum_i (\\%\\text{Good}_i - \\%\\text{Bad}_i) \\times WoE_i$$\n\nAfter WoE encoding, `purpose_WoE` and `addr_state_WoE` were **excluded** because their IV values fell below 0.02 (no predictive power threshold). The remaining encoded variables — `grade_WoE`, `home_ownership_WoE`, `verification_status_WoE` — were retained.\n\n### Engineered Feature: `loan_income_ratio`\n\nA key insight from exploratory analysis: **current debt burden relative to income is more predictive than loan size or income in isolation**. The engineered feature is:\n\n$$\\text{loan\\_income\\_ratio} = \\frac{\\text{loan\\_amnt}}{\\text{annual\\_inc}}$$\n\n| Feature | IV | Predictive Strength |\n|---------|-----|---------------------|\n| `grade_WoE` | 0.449 | Very Strong |\n| **`loan_income_ratio`** | **0.120** | **Medium** |\n| `loan_amnt` | 0.039 | Weak |\n| `annual_inc` | 0.024 | Weak |\n\nThe ratio's IV of 0.12 is **three times** that of its component features, confirming that the relative affordability metric captures risk dimensions neither feature captures alone. Values are winsorized at the 99th percentile to limit the influence of extreme outliers.\n\n### Final Feature Set — 20 Variables\n\nAfter all preprocessing, **20 features** enter the models:\n\n`loan_amnt`, `term`, `emp_length`, `annual_inc`, `dti`, `delinq_2yrs`, `fico_range_low`, `inq_last_6mths`, `open_acc`, `pub_rec`, `revol_bal`, `revol_util`, `total_acc`, `credit_history_months`, `mort_acc`, `pub_rec_bankruptcies`, `grade_WoE`, `home_ownership_WoE`, `verification_status_WoE`, `loan_income_ratio`\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude post-origination variables (generated after loan issuance — data leakage)\n",
    "leakage_cols = [\n",
    "    'loan_status',           # Target variable itself\n",
    "    'out_prncp',             # Remaining principal (only available after repayment)\n",
    "    'out_prncp_inv',\n",
    "    'total_pymnt',           # Total payment amount\n",
    "    'total_pymnt_inv',\n",
    "    'total_rec_prncp',       # Principal recovered\n",
    "    'total_rec_int',         # Interest recovered\n",
    "    'total_rec_late_fee',    # Late fees collected\n",
    "    'recoveries',            # Recovery amount\n",
    "    'collection_recovery_fee',\n",
    "    'last_pymnt_d',          # Last payment date\n",
    "    'last_pymnt_amnt',\n",
    "    'next_pymnt_d',\n",
    "    'last_credit_pull_d',\n",
    "    'last_fico_range_high',  # Most recent FICO score (updated post-origination)\n",
    "    'last_fico_range_low',\n",
    "    'funded_amnt',           # Actual funded amount\n",
    "    'funded_amnt_inv',\n",
    "    'issue_d',               # Loan issuance date\n",
    "    'url', 'desc', 'id', 'member_id'  # Non-informative ID columns\n",
    "]\n",
    "\n",
    "# Application-time features (available at underwriting — safe to use)\n",
    "application_cols = [\n",
    "    'loan_amnt',          # Requested loan amount\n",
    "    'term',               # Loan term\n",
    "    'int_rate',           # Interest rate\n",
    "    'installment',        # Monthly installment\n",
    "    'grade',              # Lending Club credit grade\n",
    "    'sub_grade',          # Sub-grade (dropped later — collinear with grade)\n",
    "    'emp_length',         # Employment length\n",
    "    'home_ownership',     # Home ownership status\n",
    "    'annual_inc',         # Annual income\n",
    "    'verification_status',# Income verification status\n",
    "    'purpose',            # Loan purpose\n",
    "    'dti',                # Debt-to-income ratio\n",
    "    'delinq_2yrs',        # Number of delinquencies in past 2 years\n",
    "    'fico_range_low',     # FICO score at application time\n",
    "    'fico_range_high',\n",
    "    'inq_last_6mths',     # Credit inquiries in last 6 months\n",
    "    'open_acc',           # Number of open credit accounts\n",
    "    'pub_rec',            # Number of derogatory public records\n",
    "    'revol_bal',          # Revolving credit balance\n",
    "    'revol_util',         # Revolving credit utilization rate\n",
    "    'total_acc',          # Total number of credit accounts\n",
    "    'addr_state',         # US state\n",
    "    'zip_code',           # ZIP code\n",
    "    'earliest_cr_line',   # Earliest credit line date\n",
    "    'mort_acc',           # Number of mortgage accounts\n",
    "    'pub_rec_bankruptcies' # Number of public record bankruptcies\n",
    "]\n",
    "\n",
    "# Build clean modelling dataset\n",
    "df_model_clean = df_model[application_cols + ['target']].copy()\n",
    "\n",
    "print(f\"Number of features: {len(application_cols)}\")\n",
    "print(f\"\\nMissing value summary:\")\n",
    "print(df_model_clean.isnull().sum()[df_model_clean.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_model_clean.copy()\n",
    "\n",
    "# Convert earliest_cr_line to credit history length in months\n",
    "import pandas as pd\n",
    "\n",
    "reference_date = pd.Timestamp('2018-12-31')  # Reference date: end of dataset\n",
    "\n",
    "df_clean['earliest_cr_line'] = pd.to_datetime(\n",
    "    df_clean['earliest_cr_line'], format='%b-%Y'\n",
    ")\n",
    "\n",
    "df_clean['credit_history_months'] = (\n",
    "    (reference_date.year - df_clean['earliest_cr_line'].dt.year) * 12 +\n",
    "    (reference_date.month - df_clean['earliest_cr_line'].dt.month)\n",
    ")\n",
    "\n",
    "# Drop original date column now that it's been converted\n",
    "df_clean = df_clean.drop(columns=['earliest_cr_line'])\n",
    "\n",
    "# Fill remaining missing values (now numeric)\n",
    "df_clean['credit_history_months'] = df_clean['credit_history_months'].fillna(\n",
    "    df_clean['credit_history_months'].median()\n",
    ")\n",
    "\n",
    "print(df_clean['credit_history_months'].describe())\n",
    "\n",
    "# 1. mort_acc and pub_rec_bankruptcies\n",
    "# Missing almost certainly means 'none on record' — fill with 0\n",
    "df_clean['mort_acc'] = df_clean['mort_acc'].fillna(0)\n",
    "df_clean['pub_rec_bankruptcies'] = df_clean['pub_rec_bankruptcies'].fillna(0)\n",
    "\n",
    "# 2. emp_length: high missingness — treat as its own 'Unknown' category\n",
    "# Do not impute with median — missingness itself may carry predictive signal\n",
    "df_clean['emp_length'] = df_clean['emp_length'].fillna('Unknown')\n",
    "\n",
    "# 3. Numeric variables with very few missing values (<120 rows) — fill with median\n",
    "small_missing = ['annual_inc', 'dti', 'delinq_2yrs', \n",
    "                 'inq_last_6mths', 'open_acc', 'pub_rec', \n",
    "                 'total_acc', 'revol_util']  # earliest_cr_line already converted\n",
    "\n",
    "for col in small_missing:\n",
    "    median_val = df_clean[col].median()\n",
    "    df_clean[col] = df_clean[col].fillna(median_val)\n",
    "\n",
    "# 4. zip_code has 1 missing row — drop it\n",
    "df_clean = df_clean.dropna(subset=['zip_code'])\n",
    "\n",
    "# Confirm no remaining missing values\n",
    "print(\"Remaining missing values:\")\n",
    "remaining = df_clean.isnull().sum()\n",
    "print(remaining[remaining > 0] if remaining[remaining > 0].any() else \"No missing values!\")\n",
    "\n",
    "print(f\"\\nFinal sample size: {len(df_clean)}\")\n",
    "print(f\"Bad Rate: {df_clean['target'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect distribution of all categorical variables\n",
    "cat_cols = ['term', 'grade', 'sub_grade', 'emp_length', \n",
    "            'home_ownership', 'verification_status', \n",
    "            'purpose', 'addr_state', 'zip_code']\n",
    "\n",
    "for col in cat_cols:\n",
    "    n = df_clean[col].nunique()\n",
    "    print(f\"{col}: {n} unique values\")\n",
    "    if n <= 10:\n",
    "        print(df_clean[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(columns=['zip_code', 'sub_grade'], errors='ignore')\n",
    "\n",
    "# 3. Merge rare home_ownership categories into 'OTHER'\n",
    "df_clean['home_ownership'] = df_clean['home_ownership'].replace({\n",
    "    'ANY': 'OTHER',\n",
    "    'NONE': 'OTHER'\n",
    "})\n",
    "print(\"home_ownership after merging rare categories:\")\n",
    "print(df_clean['home_ownership'].value_counts())\n",
    "\n",
    "# 4. Convert term to numeric (only if still stored as string)\n",
    "if df_clean['term'].dtype == object:\n",
    "    df_clean['term'] = df_clean['term'].str.extract(r'(\\d+)').astype(int)\n",
    "print(\"\\nterm after conversion to numeric:\")\n",
    "print(df_clean['term'].value_counts())\n",
    "\n",
    "# 5. Convert emp_length to ordinal numeric (only if still string)\n",
    "if df_clean['emp_length'].dtype == object:\n",
    "    emp_map = {\n",
    "        '< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3,\n",
    "        '4 years': 4, '5 years': 5, '6 years': 6, '7 years': 7,\n",
    "        '8 years': 8, '9 years': 9, '10+ years': 10, 'Unknown': -1\n",
    "    }\n",
    "    df_clean['emp_length'] = df_clean['emp_length'].map(emp_map)\n",
    "print(\"\\nemp_length after ordinal encoding:\")\n",
    "print(df_clean['emp_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_woe_iv(df, feature, target):\n",
    "    \"\"\"\n",
    "    Calculate Weight of Evidence (WoE) and Information Value (IV).\n",
    "\n",
    "    Returns:\n",
    "    - woe_df  : per-category WoE and IV breakdown\n",
    "    - iv      : overall Information Value\n",
    "    - woe_map : dictionary for mapping categories to WoE values\n",
    "    \"\"\"\n",
    "    # Build cross-tabulation\n",
    "    ct = pd.crosstab(df[feature], df[target])\n",
    "    ct.columns = ['Good', 'Bad']\n",
    "    \n",
    "    # Totals\n",
    "    total_good = ct['Good'].sum()\n",
    "    total_bad = ct['Bad'].sum()\n",
    "    \n",
    "    # Compute proportions (add small constant to avoid log(0))\n",
    "    ct['Good_Pct'] = (ct['Good'] + 0.5) / total_good\n",
    "    ct['Bad_Pct'] = (ct['Bad'] + 0.5) / total_bad\n",
    "    \n",
    "    # Compute WoE and IV\n",
    "    ct['WoE'] = np.log(ct['Good_Pct'] / ct['Bad_Pct'])\n",
    "    ct['IV_component'] = (ct['Good_Pct'] - ct['Bad_Pct']) * ct['WoE']\n",
    "    \n",
    "    # Compute Bad Rate per category\n",
    "    ct['Bad_Rate'] = ct['Bad'] / (ct['Good'] + ct['Bad'])\n",
    "    ct['Count'] = ct['Good'] + ct['Bad']\n",
    "    \n",
    "    iv = ct['IV_component'].sum()\n",
    "    woe_map = ct['WoE'].to_dict()\n",
    "    \n",
    "    return ct[['Count', 'Bad_Rate', 'Good_Pct', 'Bad_Pct', 'WoE', 'IV_component']], iv, woe_map\n",
    "\n",
    "\n",
    "# Calculate WoE and IV for all categorical variables\n",
    "cat_cols = ['grade', 'home_ownership', 'verification_status', 'purpose', 'addr_state']\n",
    "\n",
    "iv_summary = {}\n",
    "woe_maps = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    woe_df, iv, woe_map = calculate_woe_iv(df_clean, col, 'target')\n",
    "    iv_summary[col] = iv\n",
    "    woe_maps[col] = woe_map\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Variable: {col}  |  IV = {iv:.4f}\")\n",
    "    print(woe_df.sort_values('Bad_Rate', ascending=False).round(4))\n",
    "\n",
    "# Summary of IV values\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"\\nIV Summary (ranked by predictive power):\")\n",
    "iv_df = pd.DataFrame.from_dict(iv_summary, orient='index', columns=['IV'])\n",
    "iv_df = iv_df.sort_values('IV', ascending=False)\n",
    "\n",
    "def iv_strength(iv):\n",
    "    if iv < 0.02: return 'No power'\n",
    "    elif iv < 0.1: return 'Weak'\n",
    "    elif iv < 0.3: return 'Medium'\n",
    "    elif iv < 0.5: return 'Strong'\n",
    "    else: return 'Very Strong'\n",
    "\n",
    "iv_df['Strength'] = iv_df['IV'].apply(iv_strength)\n",
    "print(iv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map WoE values back into the dataset\n",
    "for col in cat_cols:\n",
    "    df_clean[f'{col}_WoE'] = df_clean[col].map(woe_maps[col])\n",
    "    \n",
    "# Drop original categorical columns\n",
    "df_clean = df_clean.drop(columns=cat_cols)\n",
    "\n",
    "# Confirm final column list\n",
    "print(\"\\nFinal column list:\")\n",
    "print(df_clean.columns.tolist())\n",
    "print(f\"\\nTotal columns (incl. target): {df_clean.shape[1]}\")\n",
    "print(f\"Sample size: {df_clean.shape[0]}\")\n",
    "\n",
    "# Confirm no missing values\n",
    "print(f\"\\nMissing values: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop WoE-encoded variables with no predictive power (IV < 0.02)\n",
    "df_clean = df_clean.drop(columns=['purpose_WoE', 'addr_state_WoE'])\n",
    "\n",
    "# 2. Check correlation between grade_WoE and int_rate\n",
    "print(\"Correlation between grade_WoE and int_rate:\")\n",
    "print(df_clean[['grade_WoE', 'int_rate']].corr())\n",
    "\n",
    "# 3. Check correlation between fico_range_low and fico_range_high\n",
    "print(\"\\nCorrelation between fico_range_low and fico_range_high:\")\n",
    "print(df_clean[['fico_range_low', 'fico_range_high']].corr())\n",
    "\n",
    "# 4. Full correlation matrix — identify highly correlated variable pairs\n",
    "corr_matrix = df_clean.drop(columns='target').corr().abs()\n",
    "\n",
    "# Identify pairs with Pearson |r| > 0.8\n",
    "high_corr = []\n",
    "cols = corr_matrix.columns\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        if corr_matrix.iloc[i, j] > 0.8:\n",
    "            high_corr.append({\n",
    "                'var1': cols[i],\n",
    "                'var2': cols[j],\n",
    "                'corr': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "print(\"\\nHighly correlated variable pairs (|r| > 0.8):\")\n",
    "print(pd.DataFrame(high_corr).sort_values('corr', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated duplicate variables\n",
    "df_clean = df_clean.drop(columns=[\n",
    "    'fico_range_high',   # Identical to fico_range_low\n",
    "    'installment',       # Highly correlated with loan_amnt\n",
    "    'int_rate'           # Highly correlated with grade_WoE; grade_WoE is more model-friendly\n",
    "])\n",
    "\n",
    "# Confirm final column list\n",
    "print(\"Final features:\")\n",
    "for col in df_clean.columns:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "print(f\"\\nTotal columns (incl. target): {df_clean.shape[1]}\")\n",
    "print(f\"Number of features: {df_clean.shape[1] - 1}\")\n",
    "\n",
    "# Double-check: no remaining high-correlation pairs\n",
    "corr_matrix = df_clean.drop(columns='target').corr().abs()\n",
    "high_corr = []\n",
    "cols = corr_matrix.columns\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        if corr_matrix.iloc[i, j] > 0.8:\n",
    "            high_corr.append({\n",
    "                'var1': cols[i],\n",
    "                'var2': cols[j],\n",
    "                'corr': round(corr_matrix.iloc[i, j], 4)\n",
    "            })\n",
    "\n",
    "if high_corr:\n",
    "    print(\"\\nStill-correlated variable pairs:\")\n",
    "    print(pd.DataFrame(high_corr).sort_values('corr', ascending=False))\n",
    "else:\n",
    "    print(\"\\nNo remaining high-correlation pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.4 Train / Test Split\n\nThe modelling sample is split **80 / 20** using `train_test_split` with `stratify=y` to preserve the class ratio in both partitions.\n\n| Partition | Rows | Bad Rate |\n|-----------|------|----------|\n| Train | 329,642 | 21.4 % |\n| Test | 82,411 | 21.4 % |\n\nStratification ensures no information leakage through class imbalance drift between splits.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df_clean.drop(columns='target')\n",
    "y = df_clean['target']\n",
    "\n",
    "# 80/20 split with stratification to preserve Bad Rate in both partitions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} rows\")\n",
    "print(f\"Test set: {X_test.shape[0]} rows\")\n",
    "print(f\"\\nTrain Bad Rate: {y_train.mean():.3f}\")\n",
    "print(f\"Test Bad Rate: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 2. Model Development\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 Model 1 — Logistic Regression\n\n### Methodology\n\nLogistic Regression serves as the primary scorecard model and the interpretability benchmark. Two preprocessing choices are applied before fitting:\n\n1. **StandardScaler** — logistic regression's gradient descent is sensitive to feature scale; standardisation ensures all 20 features contribute on equal footing.  \n2. **`class_weight='balanced'`** — automatically reweights the minority class (Bad = 1, 21.4 %) so the optimiser does not simply predict \"Good\" for every observation to minimise loss.\n\nThe decision boundary is:\n\n$$P(\\text{Bad}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p)}}$$\n\n### Results\n\n| Metric | Value |\n|--------|-------|\n| **AUC** | **0.7096** |\n| Gini | 0.4192 |\n| KS Statistic | 0.3037 |\n\n### Feature Coefficient Interpretation\n\nThe top three predictors by absolute standardised coefficient are:\n\n| Rank | Feature | Sign | Business Logic |\n|------|---------|------|----------------|\n| 1 | `grade_WoE` | + | Higher WoE = lower risk grade → lower default probability ✓ |\n| 2 | `term` | + | 60-month loans carry higher default risk than 36-month ✓ |\n| 3 | `dti` | + | Higher debt-to-income ratio → higher default risk ✓ |\n\nAll coefficient directions align with credit risk intuition, providing a sanity check on model validity.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (roc_auc_score, roc_curve, \n",
    "                             classification_report, confusion_matrix)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Standardise features (logistic regression is scale-sensitive)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Instantiate and train model\n",
    "lr = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Re-weight minority class to address imbalance\n",
    ")\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. Generate predictions\n",
    "y_pred_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "# 4. Evaluation metrics\n",
    "auc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "gini_lr = 2 * auc_lr - 1\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Logistic Regression Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUC:  {auc_lr:.4f}\")\n",
    "print(f\"Gini: {gini_lr:.4f}\")\n",
    "\n",
    "# KS Statistic\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_lr)\n",
    "ks_lr = max(tpr - fpr)\n",
    "print(f\"KS:   {ks_lr:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# 5. ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc_lr:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 6. Feature importance via coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lr.coef_[0]\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nFeature coefficients (sorted by magnitude):\")\n",
    "print(coef_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 Model 2 — XGBoost\n\n### Why XGBoost?\n\nLogistic Regression assumes a **linear relationship** between features and the log-odds of default. In reality, credit risk exhibits non-linear thresholds (e.g., DTI risk accelerates steeply above 40 %) and **interaction effects** (e.g., high DTI matters more for subprime FICO borrowers). XGBoost addresses both through sequential gradient-boosted decision trees.\n\nKey conceptual differences from Logistic Regression:\n\n| Aspect | Logistic Regression | XGBoost |\n|--------|--------------------|----|\n| Decision boundary | Linear hyperplane | Ensemble of trees |\n| Interactions | Manual (need to create terms) | Automatic |\n| Interpretability | High (coefficients) | Moderate (SHAP / feature importance) |\n| Robustness to outliers | Lower | Higher |\n| Industry standard | Scorecard-grade | Challenger model |\n\n`scale_pos_weight` is set to `len(negatives)/len(positives)` to mirror logistic regression's class weighting. Early stopping (`patience=20`) prevents overfitting on the test-set `eval_set`.\n\n### Results\n\n| Metric | Value |\n|--------|-------|\n| **AUC** | **0.7211** |\n| Gini | 0.4422 |\n| KS Statistic | 0.3219 |\n\nXGBoost improves AUC by **+1.15 pp** over Logistic Regression.\n\n### Feature Importance\n\n`grade_WoE` dominates with an importance score of **0.509**, confirming that Lending Club's own credit grade is the single most powerful predictor even in a non-linear model. `loan_income_ratio` appears in the top five, validating the value of the engineered feature.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),  # Balance class weights\n",
    "    random_state=42,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=20\n",
    ")\n",
    "\n",
    "xgb.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_proba_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "gini_xgb = 2 * auc_xgb - 1\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "ks_xgb = max(tpr_xgb - fpr_xgb)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"XGBoost Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"AUC:  {auc_xgb:.4f}\")\n",
    "print(f\"Gini: {gini_xgb:.4f}\")\n",
    "print(f\"KS:   {ks_xgb:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# ROC Curve comparison: LR vs XGBoost\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc_lr:.4f})')\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "feat_imp = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nXGBoost Feature Importance:\")\n",
    "print(feat_imp.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feat_imp['Feature'][::-1], feat_imp['Importance'][::-1], color='steelblue')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.3 Model Comparison — All Nine Models\n\nTo provide a rigorous benchmark, **nine models** were trained on the same 80 % training set and evaluated on the held-out 20 % test set.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_proba_nb = nb.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_nb, tpr_nb, _ = roc_curve(y_test, y_pred_proba_nb)\n",
    "\n",
    "auc_nb = roc_auc_score(y_test, y_pred_proba_nb)\n",
    "gini_nb = 2 * auc_nb - 1\n",
    "ks_nb = max(tpr_nb - fpr_nb)\n",
    "\n",
    "print(f\"Naive Bayes - AUC: {auc_nb:.4f} | Gini: {gini_nb:.4f} | KS: {ks_nb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "\n",
    "auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "gini_rf = 2 * auc_rf - 1\n",
    "ks_rf = max(tpr_rf - fpr_rf)\n",
    "\n",
    "print(f\"Random Forest - AUC: {auc_rf:.4f} | Gini: {gini_rf:.4f} | KS: {ks_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    callbacks=[lgb.early_stopping(20, verbose=False)]\n",
    ")\n",
    "\n",
    "y_pred_proba_lgbm = lgbm.predict_proba(X_test)[:, 1]\n",
    "fpr_lgbm, tpr_lgbm, _ = roc_curve(y_test, y_pred_proba_lgbm)\n",
    "\n",
    "auc_lgbm = roc_auc_score(y_test, y_pred_proba_lgbm)\n",
    "gini_lgbm = 2 * auc_lgbm - 1\n",
    "ks_lgbm = max(tpr_lgbm - fpr_lgbm)\n",
    "\n",
    "print(f\"LightGBM - AUC: {auc_lgbm:.4f} | Gini: {gini_lgbm:.4f} | KS: {ks_lgbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),  # Two hidden layers: 64 and 32 neurons\n",
    "    activation='relu',\n",
    "    max_iter=100,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=True\n",
    ")\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_proba_nn = nn.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_nn, tpr_nn, _ = roc_curve(y_test, y_pred_proba_nn)\n",
    "\n",
    "auc_nn = roc_auc_score(y_test, y_pred_proba_nn)\n",
    "gini_nn = 2 * auc_nn - 1\n",
    "ks_nn = max(tpr_nn - fpr_nn)\n",
    "\n",
    "print(f\"Neural Network - AUC: {auc_nn:.4f} | Gini: {gini_nn:.4f} | KS: {ks_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# LDA — fast, fit directly\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_proba_lda = lda.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_lda, tpr_lda, _ = roc_curve(y_test, y_pred_proba_lda)\n",
    "\n",
    "auc_lda = roc_auc_score(y_test, y_pred_proba_lda)\n",
    "gini_lda = 2 * auc_lda - 1\n",
    "ks_lda = max(tpr_lda - fpr_lda)\n",
    "\n",
    "print(f\"LDA - AUC: {auc_lda:.4f} | Gini: {gini_lda:.4f} | KS: {ks_lda:.4f}\")\n",
    "\n",
    "# LinearSVC — fast linear SVM implementation\n",
    "# LinearSVC does not output probabilities natively — wrap with CalibratedClassifierCV\n",
    "svm = CalibratedClassifierCV(\n",
    "    LinearSVC(\n",
    "        class_weight='balanced',\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_proba_svm = svm.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_pred_proba_svm)\n",
    "\n",
    "auc_svm = roc_auc_score(y_test, y_pred_proba_svm)\n",
    "gini_svm = 2 * auc_svm - 1\n",
    "ks_svm = max(tpr_svm - fpr_svm)\n",
    "\n",
    "print(f\"SVM (Linear) - AUC: {auc_svm:.4f} | Gini: {gini_svm:.4f} | KS: {ks_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_proba_qda = qda.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_qda, tpr_qda, _ = roc_curve(y_test, y_pred_proba_qda)\n",
    "\n",
    "auc_qda = roc_auc_score(y_test, y_pred_proba_qda)\n",
    "gini_qda = 2 * auc_qda - 1\n",
    "ks_qda = max(tpr_qda - fpr_qda)\n",
    "\n",
    "print(f\"QDA - AUC: {auc_qda:.4f} | Gini: {gini_qda:.4f} | KS: {ks_qda:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Full Comparison Table\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot in descending AUC order\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (0.7202)', linewidth=2)\n",
    "plt.plot(fpr_nn, tpr_nn, label=f'Neural Network (0.7158)', linewidth=2)\n",
    "plt.plot(fpr_lgbm3, tpr_lgbm3, label=f'LightGBM (0.7105)', linewidth=2)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (0.7103)', linewidth=2)\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (0.7101)', linewidth=2)\n",
    "plt.plot(fpr_svm, tpr_svm, label=f'SVM Linear (0.7099)', linewidth=2)\n",
    "plt.plot(fpr_lda, tpr_lda, label=f'LDA (0.7086)', linewidth=2)\n",
    "plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (0.6903)', linewidth=2)\n",
    "plt.plot(fpr_qda, tpr_qda, label=f'QDA (0.6888)', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - All Models Comparison', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Discussion — Three Performance Tiers\n\n**Tier 1 — Complex non-linear models (XGBoost, Neural Network, LightGBM, Random Forest): AUC 0.710–0.721**  \nThese models outperform linear models by capturing non-linear risk thresholds and interaction effects. The gap is modest (~1–2 pp AUC), suggesting the linear WoE-encoded features already capture most predictable signal.\n\n**Tier 2 — Linear model cluster (Logistic Regression, SVM Linear, LDA): AUC 0.708–0.710**  \nAll three produce nearly identical AUC scores. This convergence is expected: WoE encoding has already **linearised** the relationship between each feature and the log-odds of default. Given identical input transformations, any linear classifier will reach approximately the same decision boundary. The result validates the quality of the WoE preprocessing pipeline.\n\n**Tier 3 — Strong-assumption models (Naive Bayes, QDA): AUC 0.688–0.690**  \nBoth models make parametric distributional assumptions that are violated by the data. Naive Bayes assumes feature independence (clearly false — grade, DTI, and income are correlated). QDA assumes multivariate normality within each class. The penalty for these violated assumptions is ~2 pp AUC.\n\n### Impact of `loan_income_ratio` on Logistic Regression\n\nRe-training Logistic Regression **with** `loan_income_ratio` yields:\n\n| Version | AUC | Gini | KS |\n|---------|-----|------|----|\n| Without ratio | 0.7060 | 0.4120 | 0.2980 |\n| **With ratio** | **0.7096** | **0.4192** | **0.3037** |\n\nThe +0.36 pp AUC gain from a single engineered feature illustrates that **feature engineering is more impactful for linear models** than adding model complexity. Non-linear models (XGBoost) can discover interaction effects internally; linear models require explicit construction of those interactions.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create new feature\n",
    "df_clean['loan_income_ratio'] = df_clean['loan_amnt'] / df_clean['annual_inc']\n",
    "\n",
    "# 2. Basic distribution\n",
    "print(\"loan_income_ratio basic statistics:\")\n",
    "print(df_clean['loan_income_ratio'].describe().round(3))\n",
    "print(f\"\\nExtreme values (>5): {(df_clean['loan_income_ratio'] > 5).sum()}\")\n",
    "\n",
    "# 3. Assess predictive power using WoE/IV (consistent with other features)\n",
    "# Bin the ratio first\n",
    "df_clean['ratio_bin'] = pd.qcut(\n",
    "    df_clean['loan_income_ratio'].clip(upper=5),  # Winsorise extreme values\n",
    "    q=10, duplicates='drop'\n",
    ")\n",
    "\n",
    "ratio_woe = df_clean.groupby('ratio_bin').agg(\n",
    "    Good=('target', lambda x: (x==0).sum()),\n",
    "    Bad=('target', lambda x: (x==1).sum())\n",
    ").reset_index()\n",
    "\n",
    "total_good = (df_clean['target']==0).sum()\n",
    "total_bad = (df_clean['target']==1).sum()\n",
    "\n",
    "ratio_woe['pct_good'] = ratio_woe['Good'] / total_good\n",
    "ratio_woe['pct_bad'] = ratio_woe['Bad'] / total_bad\n",
    "ratio_woe['WoE'] = np.log(ratio_woe['pct_good'] / ratio_woe['pct_bad'])\n",
    "ratio_woe['IV'] = (ratio_woe['pct_good'] - ratio_woe['pct_bad']) * ratio_woe['WoE']\n",
    "ratio_woe['Bad_Rate'] = ratio_woe['Bad'] / (ratio_woe['Good'] + ratio_woe['Bad'])\n",
    "\n",
    "print(f\"\\nloan_income_ratio IV = {ratio_woe['IV'].sum():.4f}\")\n",
    "print(\"\\nBin-level detail:\")\n",
    "print(ratio_woe[['ratio_bin', 'Good', 'Bad', 'Bad_Rate', 'WoE']].round(4))\n",
    "\n",
    "# 4. Compare IV against component features\n",
    "print(\"\\n\\nIV comparison across features:\")\n",
    "iv_compare = {\n",
    "    'grade_WoE': 0.4492,\n",
    "    'loan_amnt': None,   # To be computed\n",
    "    'annual_inc': None,  # To be computed\n",
    "    'loan_income_ratio': ratio_woe['IV'].sum()\n",
    "}\n",
    "\n",
    "# Compute IV for loan_amnt and annual_inc\n",
    "for col in ['loan_amnt', 'annual_inc']:\n",
    "    bins = pd.qcut(df_clean[col], q=10, duplicates='drop')\n",
    "    ct = df_clean.groupby(bins).agg(\n",
    "        Good=('target', lambda x: (x==0).sum()),\n",
    "        Bad=('target', lambda x: (x==1).sum())\n",
    "    )\n",
    "    ct['pct_good'] = ct['Good'] / total_good\n",
    "    ct['pct_bad'] = ct['Bad'] / total_bad\n",
    "    ct['WoE'] = np.log(ct['pct_good'] / ct['pct_bad'])\n",
    "    ct['IV'] = (ct['pct_good'] - ct['pct_bad']) * ct['WoE']\n",
    "    iv_compare[col] = ct['IV'].sum()\n",
    "\n",
    "for k, v in sorted(iv_compare.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {k:25s}: {v:.4f}\")\n",
    "\n",
    "# 5. Visualise Bad Rate by loan_income_ratio bins\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(ratio_woe)), ratio_woe['Bad_Rate']*100, color='steelblue')\n",
    "plt.xticks(range(len(ratio_woe)), \n",
    "           [str(b) for b in ratio_woe['ratio_bin']], \n",
    "           rotation=45, ha='right')\n",
    "plt.xlabel('Loan/Income Ratio Bin')\n",
    "plt.ylabel('Bad Rate (%)')\n",
    "plt.title(f'Bad Rate by Loan/Income Ratio (IV = {ratio_woe[\"IV\"].sum():.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 1. Create new feature\n",
    "df_clean['loan_income_ratio'] = np.where(\n",
    "    df_clean['annual_inc'] > 0,\n",
    "    (df_clean['loan_amnt'] / df_clean['annual_inc']).clip(upper=5),\n",
    "    np.nan\n",
    ")\n",
    "df_clean['loan_income_ratio'].fillna(df_clean['loan_income_ratio'].median(), inplace=True)\n",
    "print(f\"loan_income_ratio created, median: {df_clean['loan_income_ratio'].median():.3f}\")\n",
    "\n",
    "# Drop the temporary binning column\n",
    "df_clean = df_clean.drop(columns=['ratio_bin'], errors='ignore')\n",
    "\n",
    "# Verify all columns are numeric\n",
    "print(df_clean.dtypes[df_clean.dtypes == object])  # Should be empty\n",
    "\n",
    "# 2. Build updated feature set\n",
    "X_new = df_clean.drop(columns='target')\n",
    "y_new = df_clean['target']\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "    X_new, y_new, test_size=0.2, random_state=42, stratify=y_new\n",
    ")\n",
    "\n",
    "# 3. Logistic Regression with new feature\n",
    "scaler_new = StandardScaler()\n",
    "X_train_new_scaled = scaler_new.fit_transform(X_train_new)\n",
    "X_test_new_scaled = scaler_new.transform(X_test_new)\n",
    "\n",
    "lr_new = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "lr_new.fit(X_train_new_scaled, y_train_new)\n",
    "\n",
    "y_pred_proba_lr_new = lr_new.predict_proba(X_test_new_scaled)[:, 1]\n",
    "auc_lr_new = roc_auc_score(y_test_new, y_pred_proba_lr_new)\n",
    "gini_lr_new = 2 * auc_lr_new - 1\n",
    "fpr_lr_new, tpr_lr_new, _ = roc_curve(y_test_new, y_pred_proba_lr_new)\n",
    "ks_lr_new = max(tpr_lr_new - fpr_lr_new)\n",
    "\n",
    "print(f\"\\nLR (new) - AUC: {auc_lr_new:.4f} | Gini: {gini_lr_new:.4f} | KS: {ks_lr_new:.4f}\")\n",
    "\n",
    "# 4. XGBoost with new feature\n",
    "xgb_new = XGBClassifier(\n",
    "    n_estimators=300, max_depth=5, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    scale_pos_weight=len(y_train_new[y_train_new==0]) / len(y_train_new[y_train_new==1]),\n",
    "    random_state=42, eval_metric='auc', early_stopping_rounds=20\n",
    ")\n",
    "xgb_new.fit(X_train_new, y_train_new,\n",
    "            eval_set=[(X_test_new, y_test_new)], verbose=False)\n",
    "\n",
    "y_pred_proba_xgb_new = xgb_new.predict_proba(X_test_new)[:, 1]\n",
    "auc_xgb_new = roc_auc_score(y_test_new, y_pred_proba_xgb_new)\n",
    "gini_xgb_new = 2 * auc_xgb_new - 1\n",
    "fpr_xgb_new, tpr_xgb_new, _ = roc_curve(y_test_new, y_pred_proba_xgb_new)\n",
    "ks_xgb_new = max(tpr_xgb_new - fpr_xgb_new)\n",
    "\n",
    "print(f\"XGB (new) - AUC: {auc_xgb_new:.4f} | Gini: {gini_xgb_new:.4f} | KS: {ks_xgb_new:.4f}\")\n",
    "\n",
    "# 5. Before vs after comparison\n",
    "print(f\"\\n=== Before vs After Adding loan_income_ratio ===\")\n",
    "print(f\"{'Model':<20} {'AUC':>8} {'Gini':>8} {'KS':>8}\")\n",
    "print(f\"{'LR (original)':<20} {auc_lr:>8.4f} {gini_lr:>8.4f} {ks_lr:>8.4f}\")\n",
    "print(f\"{'LR (new)':<20} {auc_lr_new:>8.4f} {gini_lr_new:>8.4f} {ks_lr_new:>8.4f}\")\n",
    "print(f\"{'XGBoost (original)':<20} {auc_xgb:>8.4f} {gini_xgb:>8.4f} {ks_xgb:>8.4f}\")\n",
    "print(f\"{'XGBoost (new)':<20} {auc_xgb_new:>8.4f} {gini_xgb_new:>8.4f} {ks_xgb_new:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.4 Business Application\n\n### Cut-off Selection Principle\n\nA model score (predicted default probability) must be converted into an **accept / reject decision** via a threshold. This threshold is called the **cut-off**. The optimal cut-off depends on the business objective:\n\n- **Maximise profit** — balance the revenue from good loans against the loss from bad loans  \n- **Minimise bad rate** — tighter cut-off, lower approval rate  \n- **Maximise volume** — looser cut-off, higher bad rate  \n\n### Business Assumptions\n\n| Parameter | Value | Rationale |\n|-----------|-------|-----------|\n| Average loan amount | \\$15,311 | Mean of sample |\n| Annual interest rate | 12 % | Approximate Lending Club average |\n| Loan term | 3 years | Dominant product term |\n| Loss Given Default (LGD) | 60 % | Industry standard for unsecured consumer lending |\n| Revenue per good loan | \\$loan × 12 % × 3 = \\$5,512 | Net interest income |\n| Loss per bad loan | \\$loan × 60 % = \\$9,187 | Principal lost after recovery |\n\n### Strategy Curve\n\nThe strategy curve plots **Acceptance Rate (%)** against **Bad Rate among Accepted (%)**. As the cut-off is relaxed (more loans approved), the bad rate rises. The curve quantifies the trade-off and allows management to set risk appetite.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.01, 1.0, 0.01)\n",
    "profits = []\n",
    "approval_rates = []\n",
    "bad_rates_approved = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Approve loans where predicted default probability <= threshold\n",
    "    approved_mask = y_pred_proba_lr <= threshold\n",
    "    \n",
    "    true_good = ((approved_mask) & (y_test == 0)).sum()\n",
    "    true_bad = ((approved_mask) & (y_test == 1)).sum()\n",
    "    \n",
    "    total_profit = (true_good * PROFIT_GOOD) - (true_bad * LOSS_BAD)\n",
    "    approval_rate = approved_mask.sum() / len(y_test)\n",
    "    bad_rate = true_bad / approved_mask.sum() if approved_mask.sum() > 0 else 0\n",
    "    \n",
    "    profits.append(total_profit)\n",
    "    approval_rates.append(approval_rate)\n",
    "    bad_rates_approved.append(bad_rate)\n",
    "\n",
    "best_idx = np.argmax(profits)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_profit = profits[best_idx]\n",
    "\n",
    "print(f\"Optimal Cut-off: {best_threshold:.2f}\")\n",
    "print(f\"Approval Rate: {approval_rates[best_idx]:.1%}\")\n",
    "print(f\"Bad Rate (Approved): {bad_rates_approved[best_idx]:.1%}\")\n",
    "print(f\"Maximum Profit: ${best_profit:,.0f}\")\n",
    "\n",
    "# English titles to avoid font rendering warnings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].plot(thresholds, [p/1e6 for p in profits], 'b-', linewidth=2)\n",
    "axes[0].axvline(x=best_threshold, color='red', linestyle='--',\n",
    "                label=f'Optimal Cut-off = {best_threshold:.2f}')\n",
    "axes[0].set_xlabel('Cut-off Threshold')\n",
    "axes[0].set_ylabel('Total Profit (Millions $)')\n",
    "axes[0].set_title('Profit vs Cut-off Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(thresholds, [r*100 for r in approval_rates],\n",
    "             'b-', linewidth=2, label='Approval Rate %')\n",
    "axes[1].plot(thresholds, [r*100 for r in bad_rates_approved],\n",
    "             'r-', linewidth=2, label='Bad Rate (Approved) %')\n",
    "axes[1].axvline(x=best_threshold, color='green', linestyle='--',\n",
    "                label=f'Optimal Cut-off = {best_threshold:.2f}')\n",
    "axes[1].set_xlabel('Cut-off Threshold')\n",
    "axes[1].set_ylabel('Rate (%)')\n",
    "axes[1].set_title('Approval Rate & Bad Rate vs Cut-off')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Business Impact Analysis - Logistic Regression', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Strategy Curve\n",
    "# X-axis: Acceptance Rate, Y-axis: Bad Rate among Approved\n",
    "axes[0].plot([r*100 for r in approval_rates],\n",
    "             [r*100 for r in bad_rates_approved],\n",
    "             'b-o', linewidth=2, markersize=3)\n",
    "\n",
    "# Mark optimal cut-off point\n",
    "best_approval = approval_rates[best_idx] * 100\n",
    "best_bad = bad_rates_approved[best_idx] * 100\n",
    "axes[0].axhline(y=best_bad, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0].axvline(x=best_approval, color='red', linestyle='--', alpha=0.7,\n",
    "                label=f'Optimal: {best_approval:.1f}% approval, {best_bad:.1f}% bad rate')\n",
    "\n",
    "axes[0].set_xlabel('Acceptance Rate (%)')\n",
    "axes[0].set_ylabel('Bad Rate among Approved (%)')\n",
    "axes[0].set_title('Strategy Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Profit Curve\n",
    "axes[1].plot(thresholds, [p/1e6 for p in profits], 'b-', linewidth=2)\n",
    "axes[1].axvline(x=best_threshold, color='red', linestyle='--',\n",
    "                label=f'Optimal Cut-off = {best_threshold:.2f}\\nProfit = ${best_profit/1e6:.1f}M')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].fill_between(thresholds, [p/1e6 for p in profits], 0,\n",
    "                     where=[p > 0 for p in profits],\n",
    "                     alpha=0.2, color='green', label='Profitable region')\n",
    "axes[1].fill_between(thresholds, [p/1e6 for p in profits], 0,\n",
    "                     where=[p <= 0 for p in profits],\n",
    "                     alpha=0.2, color='red', label='Loss region')\n",
    "\n",
    "axes[1].set_xlabel('Cut-off Threshold')\n",
    "axes[1].set_ylabel('Total Profit (Millions $)')\n",
    "axes[1].set_title('Profit vs Cut-off Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Business Impact Analysis - Logistic Regression', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Profit Curve & Optimal Cut-off\n\nThe profit curve reveals a clear maximum:\n\n| Parameter | Value |\n|-----------|-------|\n| **Optimal cut-off** | **0.69** |\n| Approval rate | 88.5 % |\n| Bad rate (approved) | ~18 % |\n| **Maximum total profit** | **~\\$195 M** |\n\nThe profit curve rises steeply as the cut-off increases from 0 (approve all → massive losses) to 0.69 (approve only low-risk borrowers). Above 0.69 the curve declines because too many profitable good borrowers are rejected.\n\nA key insight: **the profit-maximising threshold is not the threshold that minimises bad rate**. A strategy focused solely on bad rate reduction would set a much tighter cut-off, sacrificing significant revenue by rejecting borderline-but-profitable borrowers.\n\n### Run Book\n\nThe Run Book translates model scores (0–1000 scale, higher = lower risk) into operating decision bands, enabling front-line credit analysts to make consistent decisions without running the model in real time.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Run Book from LR predicted probabilities\n",
    "# Predict on full dataset (train + test combined)\n",
    "X_all_scaled = scaler_new.transform(X_new)\n",
    "y_pred_proba_all = lr_new.predict_proba(X_all_scaled)[:, 1]\n",
    "\n",
    "# Convert default probability to score (lower probability = higher score = lower risk)\n",
    "# Linear transformation to simulate a scorecard scale (0–1000)\n",
    "score_all = 1000 - (y_pred_proba_all * 1000).astype(int)\n",
    "\n",
    "# Build Run Book DataFrame\n",
    "runbook_df = pd.DataFrame({\n",
    "    'score': score_all,\n",
    "    'target': y_new.values\n",
    "}).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Bin scores into predefined bands\n",
    "bins = [0, 400, 500, 550, 600, 625, 650, 675, 700, 750, 1001]\n",
    "labels = ['<400', '400-499', '500-549', '550-599', '600-624', \n",
    "          '625-649', '650-674', '675-699', '700-749', '750+']\n",
    "\n",
    "runbook_df['score_band'] = pd.cut(runbook_df['score'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Compute statistics per band (cumulative from high score to low)\n",
    "bands = runbook_df.groupby('score_band', observed=True).agg(\n",
    "    Apps=('target', 'count'),\n",
    "    Marginal_Bads=('target', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "bands['Marginal_Bad_Rate'] = bands['Marginal_Bads'] / bands['Apps']\n",
    "bands['GB_Odds'] = (bands['Apps'] - bands['Marginal_Bads']) / bands['Marginal_Bads']\n",
    "\n",
    "# Cumulate from highest score (lowest risk) to lowest score (highest risk)\n",
    "# Acc Rate = approving all applicants above this score band\n",
    "total_apps = bands['Apps'].sum()\n",
    "total_bads = bands['Marginal_Bads'].sum()\n",
    "\n",
    "# Reverse order so highest score band appears first\n",
    "bands = bands.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "bands['Cumulative_Apps'] = bands['Apps'].cumsum()\n",
    "bands['Cumulative_Bads'] = bands['Marginal_Bads'].cumsum()\n",
    "bands['Acc_Rate'] = (bands['Cumulative_Apps'] / total_apps * 100).round(1)\n",
    "bands['Bads_among_Accepts'] = bands['Cumulative_Bads']\n",
    "bands['Bad_Rate_among_Accepts'] = (bands['Cumulative_Bads'] / bands['Cumulative_Apps'] * 100).round(2)\n",
    "bands['Goods_among_Accepts'] = bands['Cumulative_Apps'] - bands['Cumulative_Bads']\n",
    "\n",
    "# Format output columns\n",
    "runbook_display = bands[[\n",
    "    'score_band', 'Apps', 'Marginal_Bads', 'Marginal_Bad_Rate',\n",
    "    'GB_Odds', 'Acc_Rate', 'Bads_among_Accepts', \n",
    "    'Bad_Rate_among_Accepts', 'Goods_among_Accepts'\n",
    "]].copy()\n",
    "\n",
    "runbook_display.columns = [\n",
    "    'Score Band', '# Apps', 'Marginal # Bads', 'Marginal Bad Rate',\n",
    "    'G/B Odds', 'Acc Rate %', '# Bads among Accepts',\n",
    "    'Bad Rate among Accepts %', '# Goods among Accepts'\n",
    "]\n",
    "\n",
    "runbook_display['Marginal Bad Rate'] = (runbook_display['Marginal Bad Rate'] * 100).round(2)\n",
    "runbook_display['G/B Odds'] = runbook_display['G/B Odds'].round(2)\n",
    "\n",
    "print(\"Run Book — Logistic Regression Model\")\n",
    "print(\"=\"*100)\n",
    "print(runbook_display.to_string(index=False))\n",
    "\n",
    "# Mark the optimal cut-off position (threshold = 0.69)\n",
    "optimal_score = int(1000 - 0.69 * 1000)\n",
    "print(f\"\\nOptimal cut-off corresponds to score: {optimal_score}\")\n",
    "print(f\"(Default probability > 0.69 → rejected; score < {optimal_score} → rejected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 3. Limitations\n\n## 3.1 Selection Bias *(most critical)*\n\nThe most fundamental limitation is that **all model training data comes exclusively from approved loans**. Lending Club only records repayment outcomes for borrowers who passed the initial underwriting screen. Rejected applicants are never observed.\n\nThis creates a classic **survivorship bias** problem: the model is trained on a non-random, pre-filtered population. When deployed, it will be applied to the full applicant pool — including borrower profiles that the historic policy would have rejected. The model has no information about how those profiles behave.\n\nConcretely: the accepted-loan file has 151 columns; the rejected-loan file has only 9 columns with **zero overlapping variables**. There is no feasible way to bridge these datasets to correct for selection bias. Any performance metric reported in this notebook is therefore an **upper bound** on real-world model performance.\n\n## 3.2 Temporal Instability\n\nAnalysis of Bad Rate by issuance year reveals a dramatic upward trend: **13 % in 2009 → 28 % in 2018**. The training data spans the 2008–2009 Global Financial Crisis, a period of extraordinary economic stress, as well as the subsequent recovery and expansion.\n\nA model trained on this full period learns a mixture of crisis and non-crisis credit behaviour. Its predictions may be poorly calibrated for any specific economic environment — underestimating risk in downturns and overestimating risk during expansions. Walk-forward or time-stratified validation would be needed to quantify this instability.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad Rate trend by issuance year\n",
    "df_model['issue_year'] = pd.to_datetime(df_model['issue_d']).dt.year\n",
    "df_model.groupby('issue_year')['target'].mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.3 Geographic Heterogeneity\n\nThe choropleth map shows substantial variation in bad rates across US states: **Southern states reach 28 %** while **Pacific Northwest states fall as low as 15 %**. This 13 pp range suggests that local economic conditions (unemployment, housing market, industry mix) materially influence default risk.\n\n`addr_state` was excluded from the final feature set because its IV was below the 0.02 threshold after WoE encoding — likely because state-level risk is already partially captured by correlated variables such as income and FICO score. However, macro-regional economic shocks (e.g., a regional recession) would not be captured by the current model, creating a systematic blind spot.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute bad rate by state\n",
    "state_stats = df_model.groupby('addr_state').agg(\n",
    "    Total=('target', 'count'),\n",
    "    Bad=('target', 'sum')\n",
    ").reset_index()\n",
    "state_stats['Bad_Rate'] = state_stats['Bad'] / state_stats['Total'] * 100\n",
    "\n",
    "# Exclude states with fewer than 100 observations\n",
    "state_stats = state_stats[state_stats['Total'] >= 100]\n",
    "\n",
    "print(\"Top 10 states by Bad Rate:\")\n",
    "print(state_stats.nlargest(10, 'Bad_Rate')[['addr_state', 'Total', 'Bad_Rate']].round(2))\n",
    "print(\"\\nBottom 10 states by Bad Rate:\")\n",
    "print(state_stats.nsmallest(10, 'Bad_Rate')[['addr_state', 'Total', 'Bad_Rate']].round(2))\n",
    "\n",
    "# 2. Choropleth map (plotly is pre-installed on Kaggle)\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.choropleth(\n",
    "    state_stats,\n",
    "    locations='addr_state',\n",
    "    locationmode='USA-states',\n",
    "    color='Bad_Rate',\n",
    "    scope='usa',\n",
    "    color_continuous_scale='RdYlGn_r',  # Red=high bad rate, Green=low bad rate\n",
    "    title='Bad Rate by State - Lending Club 2007-2018',\n",
    "    labels={'Bad_Rate': 'Bad Rate (%)'}\n",
    ")\n",
    "fig.update_layout(\n",
    "    geo=dict(bgcolor='white'),\n",
    "    title_font_size=16\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 3. State-level averages to help explain geographic variation\n",
    "state_detail = df_model.groupby('addr_state').agg(\n",
    "    Bad_Rate=('target', lambda x: x.mean()*100),\n",
    "    Avg_FICO=('fico_range_low', 'mean'),\n",
    "    Avg_DTI=('dti', 'mean'),\n",
    "    Avg_Income=('annual_inc', 'mean'),\n",
    "    Count=('target', 'count')\n",
    ").round(2)\n",
    "\n",
    "state_detail = state_detail[state_detail['Count'] >= 100]\n",
    "state_detail = state_detail.sort_values('Bad_Rate', ascending=False)\n",
    "\n",
    "print(\"\\nDetailed statistics — top 10 highest bad rate states:\")\n",
    "print(state_detail.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.4 Simplified Profit Assumptions\n\nThe profit analysis in Section 2.4 rests on several simplifying assumptions that limit its precision as an absolute dollar forecast:\n\n- **No cost of funds** — deploying \\$15K per loan requires capital that has an opportunity cost or borrowing cost, typically 3–5 % p.a. for a P2P platform.  \n- **No operational costs** — origination, servicing, and collections costs are omitted.  \n- **No time value of money** — all cash flows are treated as occurring at origination rather than being discounted over the 3-year loan term.  \n- **Homogeneous LGD** — a flat 60 % LGD is applied to all defaults; in practice, LGD varies with collateral, recovery efforts, and macroeconomic conditions.  \n\nDespite these simplifications, the **relative comparison** across cut-offs remains valid for identifying the profit-maximising threshold. Absolute profit figures should be treated as directional estimates only.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n*End of Report*  \n*Total word count target: ~2,000 words (Data Preparation ≈ 600 · Model Development ≈ 1,200 · Limitations ≈ 200)*\n"
  }
 ]
}